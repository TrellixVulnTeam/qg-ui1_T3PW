[{"index":1,"original_sentence":"existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects . this limitation severely hinders the use of these models in real world applications dealing with images in the wild . we address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time , without re-training . our method uses constrained beam search to force the inclusion of selected tag words in the output , and fixed , pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words . using this approach we achieve state of the art results for out-of-domain captioning on mscoco -LRB- and improved results for in-domain captioning -RRB- . perhaps surprisingly , our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm . we also show that we can significantly improve the quality of generated imagenet captions by leveraging ground-truth labels . ","tagged_sentence":"existing￨O_ANS image￨O_ANS captioning￨O_ANS models￨O_ANS do￨O_ANS not￨O_ANS generalize￨O_ANS well￨O_ANS to￨O_ANS out-of-domain￨O_ANS images￨O_ANS containing￨O_ANS novel￨O_ANS scenes￨O_ANS or￨O_ANS objects￨O_ANS .￨O_ANS this￨O_ANS limitation￨O_ANS severely￨O_ANS hinders￨O_ANS the￨O_ANS use￨O_ANS of￨O_ANS these￨O_ANS models￨O_ANS in￨O_ANS real￨O_ANS world￨O_ANS applications￨O_ANS dealing￨O_ANS with￨O_ANS images￨O_ANS in￨O_ANS the￨O_ANS wild￨O_ANS .￨O_ANS we￨O_ANS address￨O_ANS this￨O_ANS problem￨O_ANS using￨O_ANS a￨O_ANS flexible￨O_ANS approach￨O_ANS that￨O_ANS enables￨O_ANS existing￨O_ANS deep￨O_ANS captioning￨O_ANS architectures￨O_ANS to￨O_ANS take￨O_ANS advantage￨O_ANS of￨O_ANS image￨B_ANS taggers￨I_ANS at￨O_ANS test￨O_ANS time￨O_ANS ,￨O_ANS without￨O_ANS re-training￨O_ANS .￨O_ANS our￨O_ANS method￨O_ANS uses￨O_ANS constrained￨O_ANS beam￨O_ANS search￨O_ANS to￨O_ANS force￨O_ANS the￨O_ANS inclusion￨O_ANS of￨O_ANS selected￨O_ANS tag￨O_ANS words￨O_ANS in￨O_ANS the￨O_ANS output￨O_ANS ,￨O_ANS and￨O_ANS fixed￨O_ANS ,￨O_ANS pretrained￨O_ANS word￨O_ANS embeddings￨O_ANS to￨O_ANS facilitate￨O_ANS vocabulary￨O_ANS expansion￨O_ANS to￨O_ANS previously￨O_ANS unseen￨O_ANS tag￨O_ANS words￨O_ANS .￨O_ANS using￨O_ANS this￨O_ANS approach￨O_ANS we￨O_ANS achieve￨O_ANS state￨O_ANS of￨O_ANS the￨O_ANS art￨O_ANS results￨O_ANS for￨O_ANS out-of-domain￨O_ANS captioning￨O_ANS on￨O_ANS mscoco￨O_ANS -LRB-￨O_ANS and￨O_ANS improved￨O_ANS results￨O_ANS for￨O_ANS in-domain￨O_ANS captioning￨O_ANS -RRB-￨O_ANS .￨O_ANS perhaps￨O_ANS surprisingly￨O_ANS ,￨O_ANS our￨O_ANS results￨O_ANS significantly￨O_ANS outperform￨O_ANS approaches￨O_ANS that￨O_ANS incorporate￨O_ANS the￨O_ANS same￨O_ANS tag￨O_ANS predictions￨O_ANS into￨O_ANS the￨O_ANS learning￨O_ANS algorithm￨O_ANS .￨O_ANS we￨O_ANS also￨O_ANS show￨O_ANS that￨O_ANS we￨O_ANS can￨O_ANS significantly￨O_ANS improve￨O_ANS the￨O_ANS quality￨O_ANS of￨O_ANS generated￨O_ANS imagenet￨O_ANS captions￨O_ANS by￨O_ANS leveraging￨O_ANS ground-truth￨O_ANS labels￨O_ANS .￨O_ANS ","answer":"image taggers","question":["What do re-training use at test time ?","What does a flexible approach provide deep captioning architectures ?","What does a flexible approach allow existing deep captioning architectures ?","What does a flexible approach provide deep captioning architectures to take ?","What does a flexible approach allow existing deep captioning architectures to take ?"],"score":[-6.253345489501953,-8.619145393371582,-9.288269996643066,-9.732393264770508,-10.610166549682617]},{"index":2,"original_sentence":"existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects . this limitation severely hinders the use of these models in real world applications dealing with images in the wild . we address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time , without re-training . our method uses constrained beam search to force the inclusion of selected tag words in the output , and fixed , pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words . using this approach we achieve state of the art results for out-of-domain captioning on mscoco -LRB- and improved results for in-domain captioning -RRB- . perhaps surprisingly , our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm . we also show that we can significantly improve the quality of generated imagenet captions by leveraging ground-truth labels . ","tagged_sentence":"existing￨O_ANS image￨O_ANS captioning￨O_ANS models￨O_ANS do￨O_ANS not￨O_ANS generalize￨O_ANS well￨O_ANS to￨O_ANS out-of-domain￨O_ANS images￨O_ANS containing￨O_ANS novel￨O_ANS scenes￨O_ANS or￨O_ANS objects￨O_ANS .￨O_ANS this￨O_ANS limitation￨O_ANS severely￨O_ANS hinders￨O_ANS the￨O_ANS use￨O_ANS of￨O_ANS these￨O_ANS models￨O_ANS in￨O_ANS real￨O_ANS world￨O_ANS applications￨O_ANS dealing￨O_ANS with￨O_ANS images￨O_ANS in￨O_ANS the￨O_ANS wild￨O_ANS .￨O_ANS we￨O_ANS address￨O_ANS this￨O_ANS problem￨O_ANS using￨O_ANS a￨O_ANS flexible￨O_ANS approach￨O_ANS that￨O_ANS enables￨O_ANS existing￨O_ANS deep￨O_ANS captioning￨O_ANS architectures￨O_ANS to￨O_ANS take￨O_ANS advantage￨O_ANS of￨O_ANS image￨O_ANS taggers￨O_ANS at￨O_ANS test￨O_ANS time￨O_ANS ,￨O_ANS without￨O_ANS re-training￨O_ANS .￨O_ANS our￨O_ANS method￨O_ANS uses￨O_ANS constrained￨O_ANS beam￨O_ANS search￨O_ANS to￨O_ANS force￨O_ANS the￨O_ANS inclusion￨O_ANS of￨O_ANS selected￨O_ANS tag￨O_ANS words￨O_ANS in￨O_ANS the￨O_ANS output￨O_ANS ,￨O_ANS and￨O_ANS fixed￨O_ANS ,￨O_ANS pretrained￨O_ANS word￨B_ANS embeddings￨I_ANS to￨O_ANS facilitate￨O_ANS vocabulary￨O_ANS expansion￨O_ANS to￨O_ANS previously￨O_ANS unseen￨O_ANS tag￨O_ANS words￨O_ANS .￨O_ANS using￨O_ANS this￨O_ANS approach￨O_ANS we￨O_ANS achieve￨O_ANS state￨O_ANS of￨O_ANS the￨O_ANS art￨O_ANS results￨O_ANS for￨O_ANS out-of-domain￨O_ANS captioning￨O_ANS on￨O_ANS mscoco￨O_ANS -LRB-￨O_ANS and￨O_ANS improved￨O_ANS results￨O_ANS for￨O_ANS in-domain￨O_ANS captioning￨O_ANS -RRB-￨O_ANS .￨O_ANS perhaps￨O_ANS surprisingly￨O_ANS ,￨O_ANS our￨O_ANS results￨O_ANS significantly￨O_ANS outperform￨O_ANS approaches￨O_ANS that￨O_ANS incorporate￨O_ANS the￨O_ANS same￨O_ANS tag￨O_ANS predictions￨O_ANS into￨O_ANS the￨O_ANS learning￨O_ANS algorithm￨O_ANS .￨O_ANS we￨O_ANS also￨O_ANS show￨O_ANS that￨O_ANS we￨O_ANS can￨O_ANS significantly￨O_ANS improve￨O_ANS the￨O_ANS quality￨O_ANS of￨O_ANS generated￨O_ANS imagenet￨O_ANS captions￨O_ANS by￨O_ANS leveraging￨O_ANS ground-truth￨O_ANS labels￨O_ANS .￨O_ANS ","answer":"word embeddings","question":["What does pretrained stand for ?","What is pretrained ?","What does re-training stand for ?","What is the pretrained ?","What is the term for pretrained ?"],"score":[-2.3564553260803223,-3.8269970417022705,-4.229936122894287,-5.298074722290039,-5.689377307891846]},{"index":3,"original_sentence":"existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects . this limitation severely hinders the use of these models in real world applications dealing with images in the wild . we address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time , without re-training . our method uses constrained beam search to force the inclusion of selected tag words in the output , and fixed , pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words . using this approach we achieve state of the art results for out-of-domain captioning on mscoco -LRB- and improved results for in-domain captioning -RRB- . perhaps surprisingly , our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm . we also show that we can significantly improve the quality of generated imagenet captions by leveraging ground-truth labels . ","tagged_sentence":"existing￨O_ANS image￨O_ANS captioning￨O_ANS models￨O_ANS do￨O_ANS not￨O_ANS generalize￨O_ANS well￨O_ANS to￨O_ANS out-of-domain￨O_ANS images￨O_ANS containing￨O_ANS novel￨O_ANS scenes￨O_ANS or￨O_ANS objects￨O_ANS .￨O_ANS this￨O_ANS limitation￨O_ANS severely￨O_ANS hinders￨O_ANS the￨O_ANS use￨O_ANS of￨O_ANS these￨O_ANS models￨O_ANS in￨O_ANS real￨O_ANS world￨O_ANS applications￨O_ANS dealing￨O_ANS with￨O_ANS images￨O_ANS in￨O_ANS the￨O_ANS wild￨O_ANS .￨O_ANS we￨O_ANS address￨O_ANS this￨O_ANS problem￨O_ANS using￨O_ANS a￨O_ANS flexible￨O_ANS approach￨O_ANS that￨O_ANS enables￨O_ANS existing￨O_ANS deep￨O_ANS captioning￨O_ANS architectures￨O_ANS to￨O_ANS take￨O_ANS advantage￨O_ANS of￨O_ANS image￨O_ANS taggers￨O_ANS at￨O_ANS test￨O_ANS time￨O_ANS ,￨O_ANS without￨O_ANS re-training￨O_ANS .￨O_ANS our￨O_ANS method￨O_ANS uses￨O_ANS constrained￨O_ANS beam￨O_ANS search￨O_ANS to￨O_ANS force￨O_ANS the￨O_ANS inclusion￨O_ANS of￨O_ANS selected￨O_ANS tag￨O_ANS words￨O_ANS in￨O_ANS the￨O_ANS output￨O_ANS ,￨O_ANS and￨O_ANS fixed￨O_ANS ,￨O_ANS pretrained￨O_ANS word￨O_ANS embeddings￨O_ANS to￨O_ANS facilitate￨O_ANS vocabulary￨B_ANS expansion￨I_ANS to￨O_ANS previously￨O_ANS unseen￨O_ANS tag￨O_ANS words￨O_ANS .￨O_ANS using￨O_ANS this￨O_ANS approach￨O_ANS we￨O_ANS achieve￨O_ANS state￨O_ANS of￨O_ANS the￨O_ANS art￨O_ANS results￨O_ANS for￨O_ANS out-of-domain￨O_ANS captioning￨O_ANS on￨O_ANS mscoco￨O_ANS -LRB-￨O_ANS and￨O_ANS improved￨O_ANS results￨O_ANS for￨O_ANS in-domain￨O_ANS captioning￨O_ANS -RRB-￨O_ANS .￨O_ANS perhaps￨O_ANS surprisingly￨O_ANS ,￨O_ANS our￨O_ANS results￨O_ANS significantly￨O_ANS outperform￨O_ANS approaches￨O_ANS that￨O_ANS incorporate￨O_ANS the￨O_ANS same￨O_ANS tag￨O_ANS predictions￨O_ANS into￨O_ANS the￨O_ANS learning￨O_ANS algorithm￨O_ANS .￨O_ANS we￨O_ANS also￨O_ANS show￨O_ANS that￨O_ANS we￨O_ANS can￨O_ANS significantly￨O_ANS improve￨O_ANS the￨O_ANS quality￨O_ANS of￨O_ANS generated￨O_ANS imagenet￨O_ANS captions￨O_ANS by￨O_ANS leveraging￨O_ANS ground-truth￨O_ANS labels￨O_ANS .￨O_ANS ","answer":"vocabulary expansion","question":["What does the word embeddings facilitate ?","What did pretrained word embeddings facilitate ?","What does the word embeddings facilitate ?","What did pretrained word embeddings facilitate ?","What did pretrained word embeddings facilitate to facilitate ?"],"score":[-4.463686466217041,-5.343047618865967,-5.490682601928711,-6.081756591796875,-7.9672956466674805]},{"index":4,"original_sentence":"existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects . this limitation severely hinders the use of these models in real world applications dealing with images in the wild . we address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time , without re-training . our method uses constrained beam search to force the inclusion of selected tag words in the output , and fixed , pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words . using this approach we achieve state of the art results for out-of-domain captioning on mscoco -LRB- and improved results for in-domain captioning -RRB- . perhaps surprisingly , our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm . we also show that we can significantly improve the quality of generated imagenet captions by leveraging ground-truth labels . ","tagged_sentence":"existing￨O_ANS image￨O_ANS captioning￨O_ANS models￨O_ANS do￨O_ANS not￨O_ANS generalize￨O_ANS well￨O_ANS to￨O_ANS out-of-domain￨O_ANS images￨O_ANS containing￨O_ANS novel￨O_ANS scenes￨O_ANS or￨O_ANS objects￨O_ANS .￨O_ANS this￨O_ANS limitation￨O_ANS severely￨O_ANS hinders￨O_ANS the￨O_ANS use￨O_ANS of￨O_ANS these￨O_ANS models￨O_ANS in￨O_ANS real￨O_ANS world￨O_ANS applications￨O_ANS dealing￨O_ANS with￨O_ANS images￨O_ANS in￨O_ANS the￨O_ANS wild￨O_ANS .￨O_ANS we￨O_ANS address￨O_ANS this￨O_ANS problem￨O_ANS using￨O_ANS a￨O_ANS flexible￨O_ANS approach￨O_ANS that￨O_ANS enables￨O_ANS existing￨O_ANS deep￨O_ANS captioning￨O_ANS architectures￨O_ANS to￨O_ANS take￨O_ANS advantage￨O_ANS of￨O_ANS image￨O_ANS taggers￨O_ANS at￨O_ANS test￨O_ANS time￨O_ANS ,￨O_ANS without￨O_ANS re-training￨O_ANS .￨O_ANS our￨O_ANS method￨O_ANS uses￨O_ANS constrained￨O_ANS beam￨O_ANS search￨O_ANS to￨O_ANS force￨O_ANS the￨O_ANS inclusion￨O_ANS of￨O_ANS selected￨O_ANS tag￨O_ANS words￨O_ANS in￨O_ANS the￨O_ANS output￨O_ANS ,￨O_ANS and￨O_ANS fixed￨O_ANS ,￨O_ANS pretrained￨O_ANS word￨O_ANS embeddings￨O_ANS to￨O_ANS facilitate￨O_ANS vocabulary￨O_ANS expansion￨O_ANS to￨O_ANS previously￨O_ANS unseen￨O_ANS tag￨O_ANS words￨O_ANS .￨O_ANS using￨O_ANS this￨O_ANS approach￨O_ANS we￨O_ANS achieve￨O_ANS state￨O_ANS of￨O_ANS the￨O_ANS art￨O_ANS results￨O_ANS for￨O_ANS out-of-domain￨O_ANS captioning￨O_ANS on￨O_ANS mscoco￨O_ANS -LRB-￨O_ANS and￨O_ANS improved￨O_ANS results￨O_ANS for￨O_ANS in-domain￨O_ANS captioning￨O_ANS -RRB-￨O_ANS .￨O_ANS perhaps￨O_ANS surprisingly￨O_ANS ,￨O_ANS our￨O_ANS results￨O_ANS significantly￨O_ANS outperform￨O_ANS approaches￨O_ANS that￨O_ANS incorporate￨O_ANS the￨O_ANS same￨O_ANS tag￨B_ANS predictions￨I_ANS into￨O_ANS the￨O_ANS learning￨O_ANS algorithm￨O_ANS .￨O_ANS we￨O_ANS also￨O_ANS show￨O_ANS that￨O_ANS we￨O_ANS can￨O_ANS significantly￨O_ANS improve￨O_ANS the￨O_ANS quality￨O_ANS of￨O_ANS generated￨O_ANS imagenet￨O_ANS captions￨O_ANS by￨O_ANS leveraging￨O_ANS ground-truth￨O_ANS labels￨O_ANS .￨O_ANS ","answer":"tag predictions","question":["What do our results significantly outperform ?","What do our results significantly outperform that incorporate ?","What do our results significantly outperform the quality of ?","What do our results significantly outperform the same as ?","What do our results significantly outperform that incorporate the same ?"],"score":[-8.60715389251709,-10.384244918823242,-10.747217178344727,-11.089896202087402,-11.441089630126953]},{"index":5,"original_sentence":"existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects . this limitation severely hinders the use of these models in real world applications dealing with images in the wild . we address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time , without re-training . our method uses constrained beam search to force the inclusion of selected tag words in the output , and fixed , pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words . using this approach we achieve state of the art results for out-of-domain captioning on mscoco -LRB- and improved results for in-domain captioning -RRB- . perhaps surprisingly , our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm . we also show that we can significantly improve the quality of generated imagenet captions by leveraging ground-truth labels . ","tagged_sentence":"existing￨O_ANS image￨O_ANS captioning￨O_ANS models￨O_ANS do￨O_ANS not￨O_ANS generalize￨O_ANS well￨O_ANS to￨O_ANS out-of-domain￨O_ANS images￨O_ANS containing￨O_ANS novel￨O_ANS scenes￨O_ANS or￨O_ANS objects￨O_ANS .￨O_ANS this￨O_ANS limitation￨O_ANS severely￨O_ANS hinders￨O_ANS the￨O_ANS use￨O_ANS of￨O_ANS these￨O_ANS models￨O_ANS in￨O_ANS real￨O_ANS world￨O_ANS applications￨O_ANS dealing￨O_ANS with￨O_ANS images￨O_ANS in￨O_ANS the￨O_ANS wild￨O_ANS .￨O_ANS we￨O_ANS address￨O_ANS this￨O_ANS problem￨O_ANS using￨O_ANS a￨O_ANS flexible￨O_ANS approach￨O_ANS that￨O_ANS enables￨O_ANS existing￨O_ANS deep￨O_ANS captioning￨O_ANS architectures￨O_ANS to￨O_ANS take￨O_ANS advantage￨O_ANS of￨O_ANS image￨O_ANS taggers￨O_ANS at￨O_ANS test￨O_ANS time￨O_ANS ,￨O_ANS without￨O_ANS re-training￨O_ANS .￨O_ANS our￨O_ANS method￨O_ANS uses￨O_ANS constrained￨O_ANS beam￨O_ANS search￨O_ANS to￨O_ANS force￨O_ANS the￨O_ANS inclusion￨O_ANS of￨O_ANS selected￨O_ANS tag￨O_ANS words￨O_ANS in￨O_ANS the￨O_ANS output￨O_ANS ,￨O_ANS and￨O_ANS fixed￨O_ANS ,￨O_ANS pretrained￨O_ANS word￨O_ANS embeddings￨O_ANS to￨O_ANS facilitate￨O_ANS vocabulary￨O_ANS expansion￨O_ANS to￨O_ANS previously￨O_ANS unseen￨O_ANS tag￨O_ANS words￨O_ANS .￨O_ANS using￨O_ANS this￨O_ANS approach￨O_ANS we￨O_ANS achieve￨O_ANS state￨O_ANS of￨O_ANS the￨O_ANS art￨O_ANS results￨O_ANS for￨O_ANS out-of-domain￨O_ANS captioning￨O_ANS on￨O_ANS mscoco￨O_ANS -LRB-￨O_ANS and￨O_ANS improved￨O_ANS results￨O_ANS for￨O_ANS in-domain￨O_ANS captioning￨O_ANS -RRB-￨O_ANS .￨O_ANS perhaps￨O_ANS surprisingly￨O_ANS ,￨O_ANS our￨O_ANS results￨O_ANS significantly￨O_ANS outperform￨O_ANS approaches￨O_ANS that￨O_ANS incorporate￨O_ANS the￨O_ANS same￨O_ANS tag￨O_ANS predictions￨O_ANS into￨O_ANS the￨O_ANS learning￨O_ANS algorithm￨O_ANS .￨O_ANS we￨O_ANS also￨O_ANS show￨O_ANS that￨O_ANS we￨O_ANS can￨O_ANS significantly￨O_ANS improve￨O_ANS the￨O_ANS quality￨O_ANS of￨O_ANS generated￨O_ANS imagenet￨B_ANS captions￨I_ANS by￨O_ANS leveraging￨O_ANS ground-truth￨O_ANS labels￨O_ANS .￨O_ANS ","answer":"imagenet captions","question":["What can leveraging ground-truth labels improve ?","What can leveraging ground-truth labels significantly improve ?","What can leveraging ground-truth labels improve the quality of ?","What can leveraging ground-truth labels improve the quality of generated ?","What can leveraging ground-truth labels improve the quality of generated by ?"],"score":[-7.928029537200928,-8.6925687789917,-9.886382102966309,-10.330779075622559,-10.357953071594238]}]